# -*- coding: utf-8 -*-
"""Text to Ontology - New Ver.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q6Ry-XKkeqmYWqsMFyrqRGeuWr07u2ky
"""

'''
!pip3 install transformers owlready2 docx2txt fasttext python-docx

!pip install docx

!pip install Pypdf2
'''

"""# Importing Libraries"""

# Commented out IPython magic to ensure Python compatibility.
import spacy
import nltk
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
import torch
import docx
from transformers import BertTokenizer, BertModel, AutoModel
from gensim.models.fasttext import FastText
import matplotlib.pyplot as plt
import string
import re
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from owlready2 import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import docx2txt
import fasttext
from transformers import logging
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import cdist
from scipy.spatial.distance import minkowski
from scipy.spatial.distance import cityblock
import hashlib
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.manifold import TSNE
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score

logging.set_verbosity_error()
# %matplotlib inline

nltk.download("punkt")
nltk.download('stopwords')
nltk.download("wordnet")
nltk.download("omw-1.4")

import warnings

# Ignore all warnings
warnings.filterwarnings('ignore')

"""# Pre-processing"""

from google.colab import files
import PyPDF2
import io
import spacy
import re

# User uploads files
uploaded = files.upload()

# File names for ontology files
file_names = ['Talenta_merge.owl']

# Dictionary to store the paths for the ontology files
ontology_paths = {}

# Write the uploaded file content to new files on the Colab filesystem
for file_name in file_names:
    if file_name in uploaded:
        file_path = f'/content/{file_name}'
        with open(file_path, 'wb') as f:
            f.write(uploaded[file_name])
        ontology_paths[file_name] = file_path
    else:
        print(f"Error: The file {file_name} was not uploaded.")

# Dictionary to store the loaded ontologies
loaded_ontologies = {}

# Load the pre-trained spaCy model
nlp = spacy.load("en_core_web_sm")

# Load the input pdf file
pdf = open(r'/content/Input_Text_1.pdf', mode='rb')
input_pdf = PyPDF2.PdfReader(pdf)

# Extract the text from the pdf file
text = ""
for page in input_pdf.pages:
    extracted_text = page.extract_text()
    if extracted_text:  # Check if text extraction returned None
        text += extracted_text + " "  # Append a space to reduce word merging

# Clean up text by replacing multiple whitespace characters with a single space
text = re.sub(r'\s+', ' ', text)

# Use the spaCy model to split the text into sentences
doc = nlp(text)
sentences = [sent.text for sent in doc.sents]

# Filter valid sentences with atleast 2 nouns and a verb
valid_sentences = []
for sent in sentences:
    doc = nlp(sent)
    nouns = [token for token in doc if token.pos_ == "NOUN"]
    verbs = [token for token in doc if token.pos_ == "VERB"]
    if len(nouns) >= 2 and len(verbs) >= 1:
        # Ensure that parentheses are balanced
        if sent.count("(") == sent.count(")"):
            # Filter out sentences with embedded formulas or composed terms that might be parsed incorrectly
            if not re.search(r"\b\w+\([\w\s]+\)|\([\w\s]+\)\w+\b", sent):
                valid_sentences.append(sent)


valid_sentences

# Define a translation table to remove punctuations, symbols, and hyphens
translator = str.maketrans("", "", string.punctuation + "’‘“”")

# Remove punctuations, symbols, and hyphens from each element of the list
valid_sentences = [s.translate(translator).strip() for s in valid_sentences]

#Removing numbers if any
valid_sentences = [re.sub(r'\d+', '', x).strip() for x in valid_sentences]

#Removing unnecessary punctuations from the text
valid_sentences = [n.replace('\n', '') for n in valid_sentences]
valid_sentences = [n.replace('\t', '') for n in valid_sentences]
valid_sentences = [n.replace('–', '') for n in valid_sentences]

valid_sentences

len(valid_sentences)

"""# Ontologies"""

from owlready2 import get_ontology

# Load the ontologies using the file paths or URLs
merger = get_ontology('/content/Talenta_merge.owl').load()

"""## Talenta_merge"""

# Get the list of classes with annotations
merger_class = []

for cls in merger.classes():
    annotations = cls.comment
    if annotations:
        merger_class.append(cls.name)

merger_class

merger.classes()
merger_cls = list(merger.classes())

merger.properties()
merger_property = list(merger.properties())

merger_annotations = []
for ann in merger_cls:
    print(f"\t{ann}: {ann.comment}")
    merger_annotations.append(str(ann.comment))

# Remove the full stop from each element
merger_annotations = [x.replace('.', '') for x in merger_annotations]

for i in range(len(merger_annotations)):
    merger_annotations[i] = merger_annotations[i] + "."

merger_annotations

merger_ann = ', '.join(merger_annotations)

# define a regular expression to match non-word characters except full stops
regex = re.compile('[%s]' % re.escape(string.punctuation.replace('.', '')))

# apply the regular expression to remove non-word characters
merger_ann = regex.sub('', merger_ann)
merger_ann = re.sub(r'\b(?<!\w)en\b(?!\w)', '', merger_ann)

merger_ann

sentences_merger = merger_ann.split('. ')
valid_sentences_merger = [s.strip() for s in sentences_merger]

# Define a translation table to remove punctuations, symbols, and hyphens
translator_merger = str.maketrans("", "", string.punctuation + "’‘“”")

# Remove punctuations, symbols, and hyphens from each element of the list
valid_sentences_merger = [s.translate(translator_merger).strip() for s in valid_sentences_merger]

#Removing unnecessary punctuations from the text
valid_sentences_merger = [n.replace('\n', '') for n in valid_sentences_merger]
valid_sentences_merger = [n.replace('locstr', '') for n in valid_sentences_merger]
valid_sentences_merger = [n.replace('-', '') for n in valid_sentences_merger]

valid_sentences_merger = [item for item in valid_sentences_merger if item != ' ' and item != '']

valid_sentences_merger

# Chexking for duplicates
unique_items = set(valid_sentences_merger)
duplicates = [item for item in unique_items if valid_sentences_merger.count(item) > 1]
duplicates

len(valid_sentences_merger)

"""### Bert Model"""

# Load pre-trained model and tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# define the get_embedding function
def get_embedding(text, model, tokenizer):
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_ids)
        last_hidden_states = outputs[0].squeeze(0)
        mean_last_hidden_states = torch.mean(last_hidden_states, dim=0)
    return mean_last_hidden_states.numpy()

# Create empty lists for embeddings
bert_embeddings = []
bert_embeddings_merger = []

# Loop over sentences and calculate embeddings
for sent1 in valid_sentences:
    embedding1 = get_embedding(sent1, model, tokenizer)
    bert_embeddings.append(embedding1)
    for sent2 in valid_sentences_merger:
        embedding2 = get_embedding(sent2, model, tokenizer)
        bert_embeddings_merger.append(embedding2)

# Create dataframe
data_merger = {
    "Input_Sentence": [],
    "Annotation": [],
    "Input_Sentence_Embedding": [],
    "Annotation_Embedding": [],
}

for sent1, emb1 in zip(valid_sentences, bert_embeddings):
    for sent2, emb2 in zip(valid_sentences_merger, bert_embeddings_merger):
        data_merger["Input_Sentence"].append(sent1)
        data_merger["Annotation"].append(sent2)
        data_merger["Input_Sentence_Embedding"].append(emb1)
        data_merger["Annotation_Embedding"].append(emb2)

df_merger = pd.DataFrame(data_merger)

df_merger.head(10)

# Adding class to the dataframe
rep_class = []
for i in range(len(valid_sentences)):
    rep_class += merger_class

df_merger['Class'] = rep_class[:len(df_merger)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_merger['Cosine_Similarity'] = df_merger.apply(cosine_sim, axis=1)

df_merger.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_merger = df_merger.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_merger = pd.merge(df_merger, max_similarities_merger, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_merger.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_merger.reset_index(drop=True, inplace=True)

df_cosine_similarity_merger.head(10)

# Create a new dataframe for hash codes
df_cosine_similarity_hc_merger = df_cosine_similarity_merger.copy()

# Removing the Embedding columns
del df_cosine_similarity_hc_merger['Input_Sentence_Embedding']
del df_cosine_similarity_hc_merger['Annotation_Embedding']

# create a new column with the hash code of the Input Sentence column
df_cosine_similarity_hc_merger['Hash_Code'] = df_cosine_similarity_hc_merger['Input_Sentence'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())

df_cosine_similarity_hc_merger.head(3)

"""### Populating the Hash-Code within the ontology"""

# Iterate through each row of the dataframe
for index, col in df_cosine_similarity_hc_merger.iterrows():
    # Get the hash code and the class from the current row
    hash_code = col['Hash_Code']
    class_name = col['Class']
    # Iterate through the classes in the ontology
    for cls in class_name:
        # Get the namespace of the ontology
        ns = merger.get_namespace(base_iri=merger.base_iri)
        # Get the class from the namespace
        cls = getattr(ns, class_name, None)
        # Check if the class was found in the ontology
        if cls is not None and isinstance(cls, ThingClass):
            # Create a new instance of the class with the hash code as the name
            instance = cls(hash_code)
        else:
            print(f"Class {class_name} not found in ontology")
        break  # exit the inner loop since we've found the matching class

# Save the ontology
merger.save("Talenta_merge.owl")

for i in merger.Entity.instances(): print(i)

# Print the individuals
for individual in merger.individuals():
    print(individual)

# Get a generator object for all individuals in the ontology
individuals = merger.individuals()

# Loop over the generator and print the URI of each individual
for individual in individuals:
    individual_uri = individual.iri
    print("Individual URI:", individual_uri)

# Iterate over all individuals in the ontology
for individual in merger.individuals():
    print("Individual:", individual.iri)
    # Get the list of classes that the individual is a direct instance of
    classes = individual.is_a
    # Print the classes
    for cls in classes:
        print("Class:", cls.iri)

"""### Fast-Text"""

# Convert the sentences to lowercase
lowercase_sentences = [sentence.lower() for sentence in valid_sentences]
lowercase_sentences_merger = [sentence.lower() for sentence in valid_sentences_merger]

# create a dictionary to map the original sentences with the converted lowercase sentences for further use
dict_merger = {}
dict_ann_merger = {}

# loop over the lists and add the elements as key-value pairs in the dictionary
for i in range(len(lowercase_sentences)):
    dict_merger[lowercase_sentences[i]] = valid_sentences[i]

for i in range(len(lowercase_sentences_merger)):
    dict_ann_merger[lowercase_sentences_merger[i]] = valid_sentences_merger[i]

# Split the sentences into words
lowercase_words = []
for sentence in lowercase_sentences:
    lowercase_words.extend(sentence.split())

lowercase_words_merger = []
for sentence in lowercase_sentences_merger:
    lowercase_words_merger.extend(sentence.split())

# Write the input sentences to a text file
with open('ft_sentences_merger.txt', 'w') as f:
    for s1 in lowercase_words:
        f.write(s1 + '\n')

# Write the annotations to a text file
with open('ft_sentences_ann_merger.txt', 'w') as f:
    for s2 in lowercase_words_merger:
        f.write(s2 + '\n')

# Create dataframe with every combination of sentences
combos = list(itertools.product(lowercase_sentences, lowercase_sentences_merger))
df_merger_ft = pd.DataFrame(combos, columns=["Input_Sentence", "Annotation"])

# Train the FastText model
ft_model_merger = fasttext.train_unsupervised('ft_sentences_merger.txt', model='skipgram', dim=100)
ft_model_ann_merger = fasttext.train_unsupervised('ft_sentences_ann_merger.txt', model='skipgram', dim=100)

# Function to create embeddings for the sentences
def create_embedding_merger(sentence):
    return ft_model_merger.get_sentence_vector(sentence)

def create_embedding_ann_merger(sentence):
    return ft_model_ann_merger.get_sentence_vector(sentence)

# Apply function to each row of the DataFrame
df_merger_ft['Input_Sentence_Embedding'] = df_merger_ft['Input_Sentence'].apply(create_embedding_merger)
df_merger_ft['Annotation_Embedding'] = df_merger_ft['Annotation'].apply(create_embedding_ann_merger)

df_merger_ft.head(10)

# Adding class to the dataframe
rep_class_ft = []
for i in range(len(lowercase_sentences)):
    rep_class_ft += merger_class

df_merger_ft['Class'] = rep_class_ft[:len(df_merger_ft)]

# Define a lambda function to calculate the cosine similarity between embeddings
cosine_sim = lambda x: cosine_similarity([x['Input_Sentence_Embedding']], [x['Annotation_Embedding']])[0][0]

# Apply the lambda function to create a new column of cosine similarities
df_merger_ft['Cosine_Similarity'] = df_merger_ft.apply(cosine_sim, axis=1)

df_merger_ft.head(10)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_ft_merger = df_merger_ft.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_cosine_similarity_ft_merger = pd.merge(df_merger_ft, max_similarities_ft_merger, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_cosine_similarity_ft_merger.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_cosine_similarity_ft_merger.reset_index(drop=True, inplace=True)

df_cosine_similarity_ft_merger.head(20)

# Removing the Embedding columns
del df_cosine_similarity_ft_merger['Input_Sentence_Embedding']
del df_cosine_similarity_ft_merger['Annotation_Embedding']

"""### Comparing Bert vs Fast-Text Models"""

# Switching every row of the 'Input_Sentence' & 'Annotation' columns of the Fast-Text dataframe to the dictionary values for comparison
df_cosine_similarity_ft_merger['Input_Sentence'] = df_cosine_similarity_ft_merger['Input_Sentence'].map(dict_merger)
df_cosine_similarity_ft_merger['Annotation'] = df_cosine_similarity_ft_merger['Annotation'].map(dict_ann_merger)

# Creating new dataframe for comparison
df_compare_merger = pd.merge(df_cosine_similarity_hc_merger, df_cosine_similarity_ft_merger, how = 'left', on = 'Input_Sentence')

# Renaming the columns
df_compare_merger.rename(columns={'Annotation_x': 'Annotation_Bert', 'Annotation_y': 'Annotation_Fast-Text', 'Class_x': 'Class_Bert',
                                 'Class_y': 'Class_Fast-Text', 'Cosine_Similarity_x': 'Cosine_Similarity_Bert', 'Cosine_Similarity_y': 'Cosine_Similarity_Fast-Text'}, inplace=True)

# Removing the unused columns
del df_compare_merger['Hash_Code']

# Changing the positons of the columns
df_compare_merger = df_compare_merger.reindex(columns=['Input_Sentence', 'Annotation_Bert', 'Annotation_Fast-Text', 'Class_Bert', 'Class_Fast-Text', 'Cosine_Similarity_Bert', 'Cosine_Similarity_Fast-Text'])

df_compare_merger.head(15)

# Plotting the results
x_merger = df_compare_merger.index.values
y1_merger = df_compare_merger['Cosine_Similarity_Bert']
y2_merger = df_compare_merger['Cosine_Similarity_Fast-Text']

plt.plot(x_merger, y1_merger, label='Bert Similarity')
plt.plot(x_merger, y2_merger, label='Fast-Text Similarity')
plt.xlabel('Index')
plt.ylabel('Cosine Similarity')
plt.title('Comparison of Cosine Similarities between Bert & Fast-Text Models in Talenta_merge Ontology')
plt.legend()
plt.show()

"""## Choosing the best ontology with Cosine Similarity"""

# Add the ontology column
df_cosine_similarity_merger['Ontology'] = 'Talenta_merge'

# Removing the Embedding columns from all dataframes
df_cosine_similarity_mason = df_cosine_similarity_merger.drop(['Input_Sentence_Embedding', 'Annotation_Embedding'], axis=1)

df_onto_merger = pd.concat([df_cosine_similarity_merger], ignore_index=True)

# group the dataframe by the sentences in the original paragraph and find the maximum cosine similarity score for each group
max_similarities_onto = df_onto_merger.groupby('Input_Sentence')['Cosine_Similarity'].max().reset_index()

# merge the original dataframe with the maximum cosine similarity scores
df_onto = pd.merge(df_onto_merger, max_similarities_onto, on=['Input_Sentence', 'Cosine_Similarity'], how='inner')

# Remove duplicates based on column 'Cosine_Similarity'
df_onto.drop_duplicates(subset=['Cosine_Similarity'], keep='first', inplace=True)

# Reset the index of the resulting dataframe
df_onto.reset_index(drop=True, inplace=True)

df_onto.head(30)

"""## Annotations"""

# Combine all the annotations and their respective ontology names into a single list
ann_list = valid_sentences_merger
ann_onto_names = ['Talenta_merge'] * len(valid_sentences_merger)

# Create a dataframe with the annotations and their ontology names
df_mlp = pd.DataFrame({'Annotation': ann_list, 'Ontology': ann_onto_names})

# Load pre-trained model and tokenizer
model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# define the get_embedding function
def get_embedding(text, model, tokenizer):
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    input_ids = torch.tensor(input_ids).unsqueeze(0)
    with torch.no_grad():
        outputs = model(input_ids)
        last_hidden_states = outputs[0].squeeze(0)
        mean_last_hidden_states = torch.mean(last_hidden_states, dim=0)
    return mean_last_hidden_states.numpy()

# Apply the Bert model to obtain annotation embeddings
df_mlp["Annotation_Embedding"] = df_mlp["Annotation"].apply(lambda x: get_embedding(x, model, tokenizer))

df_mlp.head(10)

X = df_mlp["Annotation_Embedding"].tolist()
y = df_mlp["Ontology"].tolist()

# Encode labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

input_dim = 768  # Size of BERT embeddings
hidden_dim = 768  # Size of hidden layer
output_dim = len(label_encoder.classes_)  # Number of unique labels

model_mlp = MLP(input_dim, hidden_dim, output_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_mlp.parameters(), lr=0.001)

X_train = torch.tensor(X_train)
y_train = torch.tensor(y_train)
X_test = torch.tensor(X_test)
y_test = torch.tensor(y_test)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

num_epochs = 10
batch_size = 32

for epoch in range(num_epochs):
    running_loss = 0.0
    for i in range(0, len(X_train), batch_size):
        inputs = X_train[i:i+batch_size]
        labels = y_train[i:i+batch_size]

        optimizer.zero_grad()

        outputs = model_mlp(inputs.float())
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # Calculate validation loss
    with torch.no_grad():
        val_outputs = model_mlp(X_val.float())
        val_loss = criterion(val_outputs, y_val)

    print(f'Epoch {epoch+1} - Training Loss: {running_loss/len(X_train)} - Validation Loss: {val_loss.item()}')

with torch.no_grad():
    test_outputs = model_mlp(X_test.float())
    predicted = torch.argmax(test_outputs, dim=1)
    accuracy = (predicted == y_test).sum().item() / len(y_test)
    print(f'Test Accuracy: {accuracy}')

# Convert the predicted numpy array to a PyTorch tensor
predicted = predicted.numpy()

# Compute the confusion matrix
cm = confusion_matrix(y_test, predicted)

# Create a heatmap plot
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score

# Convert the predicted tensor to a numpy array
predicted = torch.from_numpy(predicted)

# Calculate precision, recall, and F1 score
precision = precision_score(y_test, predicted, average='macro')
recall = recall_score(y_test, predicted, average='macro')
f1 = f1_score(y_test, predicted, average='macro')

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

"""## Projection of Annotation Embeddings"""

# Extract embeddings and ontology names
embeddings_ann = np.array(df_mlp["Annotation_Embedding"].tolist())
ontology_names = df_mlp["Ontology"].tolist()

# Perform t-SNE dimensionality reduction
tsne = TSNE(n_components=2, random_state=42)
embeddings_proj = tsne.fit_transform(embeddings_ann)

# Create a new DataFrame with the embeddings and ontology names
df_projection = pd.DataFrame(embeddings_proj, columns=['X', 'Y'])
df_projection["Ontology"] = ontology_names

# Plot the projection
plt.figure(figsize=(10, 8))
for ontology_name in df_projection["Ontology"].unique():
    subset = df_projection[df_projection["Ontology"] == ontology_name]
    plt.scatter(subset['X'], subset['Y'], label=ontology_name)

plt.title('Projection of Annotation Embeddings of different Ontologies')
plt.legend()
plt.show()

"""## Choosing the best ontology with MLP

### Input Text
"""

#Create a dataframe with the input sentences
df_input_mlp = pd.DataFrame({'Input_Sentence': valid_sentences})

# Apply the Bert model to obtain input sentences embeddings
df_input_mlp["Input_Sentence_Embedding"] = df_input_mlp["Input_Sentence"].apply(lambda x: get_embedding(x, model, tokenizer))

df_input_mlp.head(17)

# Convert BERT embeddings to tensors
X_new = torch.tensor(df_input_mlp['Input_Sentence_Embedding'].tolist())

with torch.no_grad():
    outputs = model_mlp(X_new.float())
    predicted_labels = label_encoder.inverse_transform(torch.argmax(outputs, dim=1))

df_input_mlp['Predicted Ontology'] = predicted_labels

df_input_mlp = df_input_mlp.drop(['Input_Sentence_Embedding'], axis=1)

df_input_mlp.head(30)

"""# Chosen Ontology by both the Methods"""

# Merge both the dataframes
df_chosen_onto = pd.merge(df_onto, df_input_mlp, on='Input_Sentence', how='left')

# Removing unecessary columns
df_chosen_onto = df_chosen_onto.drop(['Annotation', 'Class', 'Cosine_Similarity'], axis=1)

# Renaming the columns
df_chosen_onto.rename(columns={'Ontology': 'Ontology_Cosine_Similarity', 'Predicted Ontology': 'Ontology_MLP'}, inplace=True)

df_chosen_onto.head(30)